{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95952cac",
   "metadata": {},
   "source": [
    "# ADS 509 Module 1: APIs and Web Scraping\n",
    "\n",
    "This notebook has two parts. In the first part, you will scrape lyrics from AZLyrics.com. In the second part, you'll run code that verifies the completeness of your data pull. \n",
    "\n",
    "For this assignment you have chosen two musical artists who have at least 20 songs with lyrics on AZLyrics.com. We start with pulling some information and analyzing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b7ae8",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c8969e",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a47e2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for any import statements you add\n",
    "\n",
    "import shutil\n",
    "from bs4 import Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c13af3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lyrics Scrape\n",
    "\n",
    "This section asks you to pull data by scraping www.AZLyrics.com. In the notebooks where you do that work you are asked to store the data in specific ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd7df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = {'posty':\"https://www.azlyrics.com/p/postmalone.html\",\n",
    "           'slipknot':\"https://www.azlyrics.com/s/slipknot.html\"}\n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236c99b",
   "metadata": {},
   "source": [
    "## A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).) \n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again. \n",
    "\n",
    "## Part 1: Finding Links to Songs Lyrics\n",
    "\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages. \n",
    "\n",
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know? \n",
    "\n",
    "A: The webscraping we are about to do is allowed because only two things are disallowed on the robots.txt page for azlyrics.com. This includes /lyricsdb/ and /song/. We are not using either page, so what we are about to preform is acceptable. All crawlers on azlyrics.com have these rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddf2312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing lyrics pages for: posty\n",
      "Processing https://www.azlyrics.com/p/postmalone.html\n",
      "Grabbing lyrics pages for: slipknot\n",
      "Processing https://www.azlyrics.com/s/slipknot.html\n"
     ]
    }
   ],
   "source": [
    "# Set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "# Iterate over each artist and their respective artist page\n",
    "for artist, artist_page in artists.items():\n",
    "    print(f\"Grabbing lyrics pages for: {artist}\")\n",
    "\n",
    "    # Request the artist's page\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10 * random.random())  # Sleep to avoid overwhelming the server\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    print(f\"Processing {artist_page}\")\n",
    "\n",
    "    # Find all links (<a> tags) to lyrics pages inside divs with class 'listalbum-item'\n",
    "    for div in soup.find_all('div', class_='listalbum-item'):\n",
    "        a_tag = div.find('a', href=True)  # Find the <a> tag inside the div\n",
    "\n",
    "        if a_tag:\n",
    "            href = a_tag['href']\n",
    "\n",
    "            # Check if the href ends with '.html' (a lyrics page) and starts with '/'\n",
    "            if href.endswith('.html') and href.startswith('/'):\n",
    "                full_url = f\"https://www.azlyrics.com{href}\"\n",
    "                lyrics_pages[artist].append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1351d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posty: ['https://www.azlyrics.com/lyrics/postmalone/neverunderstand.html', 'https://www.azlyrics.com/lyrics/postmalone/moneymademedoit.html', 'https://www.azlyrics.com/lyrics/postmalone/gitwitu.html', 'https://www.azlyrics.com/lyrics/postmalone/goddamn.html', 'https://www.azlyrics.com/lyrics/postmalone/fuck.html']\n",
      "slipknot: ['https://www.azlyrics.com/lyrics/slipknot/slipknot.html', 'https://www.azlyrics.com/lyrics/slipknot/gently-1996.html', 'https://www.azlyrics.com/lyrics/slipknot/donothingbitchslap.html', 'https://www.azlyrics.com/lyrics/slipknot/onlyone.html', 'https://www.azlyrics.com/lyrics/slipknot/tatteredandtorn48176.html']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 links for each artist\n",
    "for artist, links in lyrics_pages.items():\n",
    "    print(f\"{artist}: {links[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c285ec1",
   "metadata": {},
   "source": [
    "Let's make sure we have enough lyrics pages to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae4cda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edca10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For posty we have 148.\n",
      "The full pull will take for this artist will take 0.41 hours.\n",
      "For slipknot we have 132.\n",
      "The full pull will take for this artist will take 0.37 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f150301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Artist: posty\n",
      "https://www.azlyrics.com/lyrics/postmalone/neverunderstand.html\n",
      "https://www.azlyrics.com/lyrics/postmalone/moneymademedoit.html\n",
      "https://www.azlyrics.com/lyrics/postmalone/gitwitu.html\n",
      "https://www.azlyrics.com/lyrics/postmalone/goddamn.html\n",
      "https://www.azlyrics.com/lyrics/postmalone/fuck.html\n",
      "\n",
      "Artist: slipknot\n",
      "https://www.azlyrics.com/lyrics/slipknot/slipknot.html\n",
      "https://www.azlyrics.com/lyrics/slipknot/gently-1996.html\n",
      "https://www.azlyrics.com/lyrics/slipknot/donothingbitchslap.html\n",
      "https://www.azlyrics.com/lyrics/slipknot/onlyone.html\n",
      "https://www.azlyrics.com/lyrics/slipknot/tatteredandtorn48176.html\n"
     ]
    }
   ],
   "source": [
    "# Display the populated lyrics pages dictionary\n",
    "for artist, song_urls in lyrics_pages.items():\n",
    "    print(f\"\\nArtist: {artist}\")\n",
    "    for url in song_urls[:5]:  # Show the first 5 URLs to keep output short\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011be6c6",
   "metadata": {},
   "source": [
    "## Part 2: Pulling Lyrics\n",
    "\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part. \n",
    "\n",
    "1. Create an empty folder in our repo called \"lyrics\". \n",
    "1. Iterate over the artists in `lyrics_pages`. \n",
    "1. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have `lyrics/cher/` in your repo.\n",
    "1. Iterate over the pages. \n",
    "1. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "1. Use the function below, `generate_filename_from_url`, to create a filename based on the lyrics page, then write the lyrics to a text file with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67693711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link):\n",
    "    if not link:\n",
    "        return None\n",
    "    \n",
    "    # Remove the protocol\n",
    "    link = link.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    \n",
    "    # Remove the \".html\" suffix\n",
    "    link = link.replace(\".html\", \"\")\n",
    "    \n",
    "    # Find the last occurrence of \"/\" and get the part after it\n",
    "    last_slash_index = link.rfind(\"/\")\n",
    "    if last_slash_index != -1:\n",
    "        # Extract the part after the last \"/\"\n",
    "        name = link[last_slash_index + 1:]\n",
    "    else:\n",
    "        # If there is no \"/\", return None or handle as needed\n",
    "        return None\n",
    "    \n",
    "    # Replace problematic characters with underscores\n",
    "    name = name.replace(\":\", \"_\").replace(\"?\", \"_\")\n",
    "    \n",
    "    # Add the \".txt\" extension\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93294e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and extract the song title\n",
    "def extract_title(raw_title):\n",
    "    # Extract the part between \" - \" and \" Lyrics\"\n",
    "    if \" - \" in raw_title and \"Lyrics\" in raw_title:\n",
    "        start = raw_title.index(\" - \") + 3\n",
    "        end = raw_title.index(\" Lyrics\")\n",
    "        return raw_title[start:end].strip()\n",
    "    return raw_title.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5e3ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the lyrics by removing the comment\n",
    "def clean_lyrics(lyrics_div):\n",
    "    # Remove comments (including the \"Usage of azlyrics.com content\" comment)\n",
    "    for element in lyrics_div(text=lambda text: isinstance(text, Comment)):\n",
    "        element.extract()\n",
    "    \n",
    "    # Return the clean text from the div\n",
    "    return lyrics_div.get_text(separator=\"\\n\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a622df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'C:/Users/benog/OneDrive/Documents/Grad School/USD/ADS 509 Text Mining/lyrics' already exists. \n",
      "Removing it and creating a new one.\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path\n",
    "folder_name = \"C:/Users/benog/OneDrive/Documents/Grad School/USD/ADS 509 Text Mining/lyrics\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.isdir(folder_name):\n",
    "    print(f\"'{folder_name}' already exists \\nRemoving it and creating a new one\")\n",
    "    shutil.rmtree(folder_name)\n",
    "\n",
    "# Create the new folder\n",
    "os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbaf8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'C:/Users/benog/OneDrive/Documents/Grad School/USD/ADS 509 Text Mining/lyrics/posty' exists. Removing it and creating a new one.\n",
      "Total pages fetched: 0\n",
      "Time taken: 2941.5527362823486 seconds\n"
     ]
    }
   ],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "# Step to flatten the list of links for song lyrics\n",
    "for artist, urls in lyrics_pages.items():\n",
    "    # Flatten the list of links if there are any lists inside the urls\n",
    "    lyrics_pages[artist] = [item for sublist in urls for item in sublist] if any(isinstance(i, list) for i in urls) else urls\n",
    "\n",
    "# Extract song data from each artist\n",
    "for artist, url_list in lyrics_pages.items():\n",
    "    # Create a subfolder for the artist\n",
    "    artist_folder = os.path.join(folder_name, artist).replace('\\\\', '/')\n",
    "\n",
    "    # Check if the folder exists, remove and recreate it if needed\n",
    "    if os.path.isdir(artist_folder):\n",
    "        print(f\"Folder '{artist_folder}' exists. Removing it and creating a new one.\")\n",
    "        shutil.rmtree(artist_folder)\n",
    "\n",
    "    os.mkdir(artist_folder)\n",
    "\n",
    "    # Iterate over the lyrics pages, request the page, extract title and lyrics, and save the data\n",
    "    for i, song_url in enumerate(url_list):\n",
    "\n",
    "        # Request the lyrics page\n",
    "        response = requests.get(song_url)\n",
    "        time.sleep(5 + 10 * random.random())  # Sleep to avoid overwhelming the server\n",
    "\n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the song title\n",
    "        raw_title = soup.find('title').text.strip()\n",
    "        title = extract_title(raw_title) \n",
    "\n",
    "        # Find the comment preceding the lyrics\n",
    "        comment = soup.find(string=lambda text: isinstance(text, Comment) and \"Usage of azlyrics.com content\" in text)\n",
    "\n",
    "        if comment:\n",
    "            # Find the div that contains the lyrics after the comment\n",
    "            lyrics_div = comment.find_parent(\"div\")\n",
    "            if lyrics_div:\n",
    "                # Clean the lyrics by removing the comment\n",
    "                lyrics = clean_lyrics(lyrics_div)\n",
    "            else:\n",
    "                lyrics = \"Lyrics not found\"\n",
    "        else:\n",
    "            lyrics = \"Lyrics not found\"\n",
    "\n",
    "        # Write the title and lyrics to a file\n",
    "        filename = generate_filename_from_link(song_url)\n",
    "        file_path = os.path.join(artist_folder, filename).replace('\\\\', '/')\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{title}\\n\\n{lyrics}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total pages fetched: {total_pages}\")\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36c394f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 1.0 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cf14b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "This assignment asks you to pull data by scraping www.AZLyrics.com.  After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "217c2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37778a1c",
   "metadata": {},
   "source": [
    "## Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bccac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For posty we have 147 files.\n",
      "For posty we have roughly 56180 words, 3824 are unique.\n",
      "For slipknot we have 132 files.\n",
      "For slipknot we have roughly 43801 words, 3570 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
